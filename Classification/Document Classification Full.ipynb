{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d213e90d039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m  \u001b[0mxgboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> 0390795af5ab677703548136401898fedd3010c2
   "source": [
    "# Import Libraries # \n",
    "import pandas as pd \n",
    "import nltk as nltk\n",
    "import numpy, string\n",
    "import datetime as dt\n",
    "import re as re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions # \n",
    "\n",
    "# Timer to check execution timing for each function call # \n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = dt.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((dt.datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "def stem(array):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in array]\n",
    "\n",
    "def lemmetize(array):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    return [lemmatizer.lemmatize(w) for w in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Data Prep # \n",
    "df = pd.read_csv(\"../data/cleaned_hotelreviews.csv\")\n",
    "\n",
    "# Drop rows with null comments # \n",
    "df = df.dropna(subset=['reviews'])\n",
    "\n",
    "# Make words case-insensitive # \n",
    "df = df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Accuracy:  0.9275155555022719\n",
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     71747\n",
      "           1       0.96      0.91      0.93     95075\n",
      "\n",
      "    accuracy                           0.93    166822\n",
      "   macro avg       0.92      0.93      0.93    166822\n",
      "weighted avg       0.93      0.93      0.93    166822\n",
      "\n",
      "true negative:  68069 \n",
      "\n",
      "false positive:  3678 \n",
      "\n",
      "false negative:  8414 \n",
      "\n",
      "true positive:  86661 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  0.9391147450576063\n",
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     71747\n",
      "           1       0.95      0.94      0.95     95075\n",
      "\n",
      "    accuracy                           0.94    166822\n",
      "   macro avg       0.94      0.94      0.94    166822\n",
      "weighted avg       0.94      0.94      0.94    166822\n",
      "\n",
      "true negative:  67092 \n",
      "\n",
      "false positive:  4655 \n",
      "\n",
      "false negative:  5502 \n",
      "\n",
      "true positive:  89573 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:  0.9245303377252401\n",
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91     71747\n",
      "           1       0.95      0.91      0.93     95075\n",
      "\n",
      "    accuracy                           0.92    166822\n",
      "   macro avg       0.92      0.93      0.92    166822\n",
      "weighted avg       0.93      0.92      0.92    166822\n",
      "\n",
      "true negative:  67384 \n",
      "\n",
      "false positive:  4363 \n",
      "\n",
      "false negative:  8227 \n",
      "\n",
      "true positive:  86848 \n",
      "\n",
      "Naive Bayes Accuracy:  0.9193391758880723\n",
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     71747\n",
      "           1       0.92      0.94      0.93     95075\n",
      "\n",
      "    accuracy                           0.92    166822\n",
      "   macro avg       0.92      0.92      0.92    166822\n",
      "weighted avg       0.92      0.92      0.92    166822\n",
      "\n",
      "true negative:  64305 \n",
      "\n",
      "false positive:  7442 \n",
      "\n",
      "false negative:  6014 \n",
      "\n",
      "true positive:  89061 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run baseline models based on Count Vector # \n",
    "\n",
    "# Train - Test Split # \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['reviews'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable # \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# Create Count Vector #  \n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['reviews'])\n",
    "\n",
    "# Transform training and validation data # \n",
    "xtrain_count = count_vector.transform(train_x)\n",
    "xvalid_count = count_vector.transform(valid_x)\n",
    "\n",
    "# XG Boost on Unprocessed Data # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_count.tocsc(), train_y)\n",
    "predictions = model.predict(xvalid_count.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"XGBoost Classifier Accuracy: \", accuracy)\n",
    "tn,fp,fn,tp = confusion_matrix(valid_y, predictions).ravel()\n",
    "print('\\nClasification report:\\n', classification_report(valid_y, predictions))\n",
    "print(\"true negative: \", tn, \"\\n\")\n",
    "print(\"false positive: \", fp, \"\\n\")\n",
    "print(\"false negative: \", fn, \"\\n\")\n",
    "print(\"true positive: \", tp, \"\\n\")\n",
    "\n",
    "# Logistic Regression on Unprocessed Data # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Logistic Regression Accuracy: \", accuracy)\n",
    "tn,fp,fn,tp = confusion_matrix(valid_y, predictions).ravel()\n",
    "print('\\nClasification report:\\n', classification_report(valid_y, predictions))\n",
    "print(\"true negative: \", tn, \"\\n\")\n",
    "print(\"false positive: \", fp, \"\\n\")\n",
    "print(\"false negative: \", fn, \"\\n\")\n",
    "print(\"true positive: \", tp, \"\\n\")\n",
    "\n",
    "# SVM on Unprocessed Data # \n",
    "#model = svm.SVC().fit(xtrain_count, train_y)\n",
    "#predictions = model.predict(xvalid_count)\n",
    "#accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "#print(\"SVM Accuracy: \", accuracy)\n",
    "\n",
    "# Random Forest on Unprocessed Data # \n",
    "model  = ensemble.RandomForestClassifier().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Random Forest Accuracy: \", accuracy)\n",
    "tn,fp,fn,tp = confusion_matrix(valid_y, predictions).ravel()\n",
    "print('\\nClasification report:\\n', classification_report(valid_y, predictions))\n",
    "print(\"true negative: \", tn, \"\\n\")\n",
    "print(\"false positive: \", fp, \"\\n\")\n",
    "print(\"false negative: \", fn, \"\\n\")\n",
    "print(\"true positive: \", tp, \"\\n\")\n",
    "\n",
    "# Naive Bayes on Unprocessed Data # \n",
    "model = naive_bayes.MultinomialNB().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Naive Bayes Accuracy: \", accuracy)\n",
    "tn,fp,fn,tp = confusion_matrix(valid_y, predictions).ravel()\n",
    "print('\\nClasification report:\\n', classification_report(valid_y, predictions))\n",
    "print(\"true negative: \", tn, \"\\n\")\n",
    "print(\"false positive: \", fp, \"\\n\")\n",
    "print(\"false negative: \", fn, \"\\n\")\n",
    "print(\"true positive: \", tp, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Data Prep on two best models # \n",
    "\n",
    "# Remove punctuations if any # \n",
    "df[\"words_only\"] = df['reviews'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Remove stop words # \n",
    "stop_list = stopwords.words('english')\n",
    "df['reviews'] = df['reviews'].apply(lambda x: [word for word in x.split() if word not in stop_list])\n",
    "\n",
    "# Remove single words # \n",
    "df['reviews'] = df['reviews'].apply(lambda x: x if len(x) > 1 else [])\n",
    "\n",
    "# Drop rows where reviews == [] # \n",
    "df = df[df.reviews.str.len()>0]\n",
    "\n",
    "# Tokenization with NLTK # \n",
    "df['tokenized'] = df['words_only'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Stemming with NLTK # \n",
    "df['stemmed'] = df['tokenized'].apply(stem)\n",
    "\n",
    "# Turn arrays for each row in df['stemmed'] into a string #: Needed to run SkLearn Lib\n",
    "df['stemmed'] = df['stemmed'].apply(\" \".join)\n",
    "\n",
    "# Lemmetisation # \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "df['lemmetized'] = df['tokenized'].apply(lemmetize)\n",
    "\n",
    "# Turn arrays for each row in df['lemmetized'] into a string #: Needed to run SkLearn Lib\n",
    "df['lemmetized'] = df['lemmetized'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test Split for Stemmed Words # \n",
    "train_stemx, valid_stemx, train_stemy, valid_stemy = model_selection.train_test_split(df['stemmed'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable [STEMMED] # \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_stemy = encoder.fit_transform(train_stemy)\n",
    "valid_stemy = encoder.fit_transform(valid_stemy)\n",
    "\n",
    "# Train - Test Split for Lemmetized Words # \n",
    "train_lemx, valid_lemx, train_lemy, valid_lemy = model_selection.train_test_split(df['lemmetized'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable [LEMMETIZED] # \n",
    "train_lemy = encoder.fit_transform(train_lemy)\n",
    "valid_lemy = encoder.fit_transform(valid_lemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Stem:  0.9453969978772869\n",
      "XGBoost Stem:  0.9357437076720914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Unigram Stem:  0.9475765692914182\n",
      "XGBoost Unigram Stem:  0.9364702314768018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Bigram Stem:  0.9264631557667037\n",
      "XGBoost Bigram Stem:  0.8768257859092288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Trigram Stem:  0.8366142727180835\n",
      "XGBoost Trigram Stem:  0.8768257859092288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Quadgram Stem:  0.7281411098756697\n",
      "XGBoost Quadgram Stem:  0.6443823915900131\n"
     ]
    }
   ],
   "source": [
    "# Best Models [STEMMED] # \n",
    "\n",
    "# Create Count Vector [STEMMED] #  \n",
    "count_vector.fit(df['stemmed'])\n",
    "xtrain_count_stem = count_vector.transform(train_stemx)\n",
    "xvalid_count_stem = count_vector.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_count_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_count_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_count_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_count_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Stem: \", accuracy)\n",
    "\n",
    "# Ngram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_quadgram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(4,4), max_features=5000)\n",
    "\n",
    "# Unigram [STEMMED] # \n",
    "tfidf_unigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_unigram_stem = tfidf_unigram.transform(train_stemx)\n",
    "xvalid_tfidf_unigram_stem = tfidf_unigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_unigram_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Unigram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_unigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Unigram Stem: \", accuracy)\n",
    "\n",
    "# Bigram [STEMMED] # \n",
    "tfidf_bigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_bigram_stem = tfidf_bigram.transform(train_stemx)\n",
    "xvalid_tfidf_bigram_stem = tfidf_bigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_bigram_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Bigram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_bigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Bigram Stem: \", accuracy)\n",
    "\n",
    "# Trigram [STEMMED] # \n",
    "tfidf_trigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_trigram_stem = tfidf_trigram.transform(train_stemx)\n",
    "xvalid_tfidf_trigram_stem = tfidf_trigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_trigram_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Trigram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_bigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Trigram Stem: \", accuracy)\n",
    "\n",
    "# Quadgram [STEMMED] # \n",
    "tfidf_quadgram.fit(df['stemmed'])\n",
    "xtrain_tfidf_quadgram_stem = tfidf_quadgram.transform(train_stemx)\n",
    "xvalid_tfidf_quadgram_stem = tfidf_quadgram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_quadgram_stem, train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Quadgram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_quadgram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"XGBoost Quadgram Stem: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Lem:  0.9464899423835035\n",
      "XGBoost Lem:  0.9342211664813505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Unigram Lem:  0.9490991104821591\n",
      "XGBoost Unigram Lem:  0.935718437278884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Bigram Lem:  0.9252122713029415\n",
      "Xgboost Bigram Lem:  0.8740081370666127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Trigram Lem:  0.8354202466390377\n",
      "XGBoost Trigram Lem:  0.7396328211866977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Quadgram Lem:  0.7264922167188922\n",
      "XGBoost Quadgram Lem:  0.6434031638532296\n"
     ]
    }
   ],
   "source": [
    "# Best Models [LEMMETIZED] # \n",
    "\n",
    "# Create Count Vector [LEMMETIZED] #  \n",
    "count_vector.fit(df['lemmetized'])\n",
    "xtrain_count_lem = count_vector.transform(train_lemx)\n",
    "xvalid_count_lem = count_vector.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_count_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_count_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_count_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_count_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"XGBoost Lem: \", accuracy)\n",
    "\n",
    "# Ngram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_quadgram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(4,4), max_features=5000)\n",
    "\n",
    "# Unigram [LEMMETIZED] # \n",
    "tfidf_unigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_unigram_lem = tfidf_unigram.transform(train_lemx)\n",
    "xvalid_tfidf_unigram_lem = tfidf_unigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_unigram_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Unigram Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_unigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"XGBoost Unigram Lem: \", accuracy)\n",
    "\n",
    "# Bigram [LEMMETIZED] # \n",
    "tfidf_bigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_bigram_lem = tfidf_bigram.transform(train_lemx)\n",
    "xvalid_tfidf_bigram_lem = tfidf_bigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_bigram_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Bigram Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_bigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Xgboost Bigram Lem: \", accuracy)\n",
    "\n",
    "# Trigram [LEMMETIZED] # \n",
    "tfidf_trigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_trigram_lem = tfidf_trigram.transform(train_lemx)\n",
    "xvalid_tfidf_trigram_lem = tfidf_trigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_trigram_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Trigram Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_trigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"XGBoost Trigram Lem: \", accuracy)\n",
    "\n",
    "# Quadgram [LEMMETIZED] # \n",
    "tfidf_quadgram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_quadgram_lem = tfidf_quadgram.transform(train_lemx)\n",
    "xvalid_tfidf_quadgram_lem = tfidf_quadgram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf_quadgram_lem, train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_lem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Logistic Regression Quadgram Lem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_tfidf_quadgram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"XGBoost Quadgram Lem: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Unigram Stem:  0.9475765692914182\n"
     ]
    }
   ],
   "source": [
    "# Unigram [STEMMED] # \n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_unigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_unigram_stem = tfidf_unigram.transform(train_stemx)\n",
    "xvalid_tfidf_unigram_stem = tfidf_unigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model_1 = linear_model.LogisticRegression().fit(xtrain_tfidf_unigram_stem, train_stemy)\n",
    "predictions = model_1.predict(xvalid_tfidf_unigram_stem)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Logistic Regression Unigram Stem: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "#model_2 = xgboost.XGBClassifier().fit(xtrain_tfidf_unigram_stem.tocsc(), train_stemy)\n",
    "#predictions = model_2.predict(xvalid_tfidf_unigram_stem.tocsc())\n",
    "#accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "#print(\"XGBoost Unigram Stem: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model for Future Use, Pickle # \n",
    "import pickle \n",
    "\n",
    "# Save the model to disk # \n",
    "filename1 = 'classification_logreg.pkl'\n",
    "with open(filename1, 'wb') as file:  \n",
    "    pickle.dump([model_1,tfidf_unigram], file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Model # \n",
    "filename1 = 'classification2.pkl'\n",
    "\n",
    "with open(filename1, 'rb') as file:  \n",
    "    logistic = pickle.load(file)[0]\n",
    "    \n",
    "with open(filename1, 'rb') as file:  \n",
    "    user_uni = pickle.load(file)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-dd906130c4c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtfidf_unigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'\\w{1,}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtfidf_unigram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stemmed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mxtrain_tfidf_unigram_stem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_unigram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_stemx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mxvalid_tfidf_unigram_stem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_unigram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_stemx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents, copy)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_tfidf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'The tfidf vector is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1681\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create Count Vector [STEMMED] #  \n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_unigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_unigram_stem = tfidf_unigram.transform(train_stemx)\n",
    "xvalid_tfidf_unigram_stem = tfidf_unigram.transform(valid_stemx)\n",
    "\n",
    "logistic_pred = logistic.predict(xvalid_tfidf_unigram_stem)\n",
    "xgboost_pred = xgboost.predict(xvalid_tfidf_unigram_stem.tocsc())\n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(valid_stemy, logistic_pred).ravel()\n",
    "accuracy = metrics.accuracy_score(logistic_pred, valid_stemy)\n",
    "print(\"Logistic Regression Unigram Stem: \", accuracy)\n",
    "print('\\nClasification report:\\n', classification_report(valid_stemy, logistic_pred))\n",
    "print(\"true negative: \", tn, \"\\n\")\n",
    "print(\"false positive: \", fp, \"\\n\")\n",
    "print(\"false negative: \", fn, \"\\n\")\n",
    "print(\"true positive: \", tp, \"\\n\")\n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(valid_stemy, xgboost_pred).ravel()\n",
    "accuracy = metrics.accuracy_score(xgboost_pred, valid_stemy)\n",
    "print(\"XGBoost Unigram Stem: \", accuracy)\n",
    "print('\\nClasification report:\\n', classification_report(valid_stemy, xgboost_pred))\n",
    "print(\"true negative: \", tn, \"\\n\")\n",
    "print(\"false positive: \", fp, \"\\n\")\n",
    "print(\"false negative: \", fn, \"\\n\")\n",
    "print(\"true positive: \", tp, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_time = timer(None)\n",
    "\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=linear_model.LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(xtrain_tfidf_unigram_stem,train_stemy)\n",
    "logreg_pred = logreg_cv.predict(xvalid_tfidf_unigram_stem)\n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(valid_stemy, logreg_pred).ravel()\n",
    "accuracy = metrics.accuracy_score(logreg_pred, valid_stemy)\n",
    "print(\"Logistic Regression with Tuned Hyperparameters Accuracy: \", accuracy)\n",
    "print('\\nClasification report:\\n', classification_report(valid_stemy, logreg_pred))\n",
    "print(\"true negative: \", tn, \"\\n\")\n",
    "print(\"false positive: \", fp, \"\\n\")\n",
    "print(\"false negative: \", fn, \"\\n\")\n",
    "print(\"true positive: \", tp, \"\\n\")\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)\n",
    "\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with Tuned Hyperparameters Accuracy:  0.9484609516529232\n",
      "\n",
      "Clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     68827\n",
      "           1       0.96      0.94      0.95     89461\n",
      "\n",
      "    accuracy                           0.95    158288\n",
      "   macro avg       0.95      0.95      0.95    158288\n",
      "weighted avg       0.95      0.95      0.95    158288\n",
      "\n",
      "true negative:  65498 \n",
      "\n",
      "false positive:  3329 \n",
      "\n",
      "false negative:  4969 \n",
      "\n",
      "true positive:  84492 \n",
      "\n",
      "tuned hpyerparameters :(best parameters)  {'C': 1.0, 'penalty': 'l2'}\n",
      "accuracy : 0.9484609516529232\n"
     ]
    }
   ],
   "source": [
    "logreg_pred = logreg_cv.best_estimator_.predict(xvalid_tfidf_unigram_stem)\n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(valid_stemy, logreg_pred).ravel()\n",
    "accuracy = metrics.accuracy_score(logreg_pred, valid_stemy)\n",
    "print(\"Logistic Regression with Tuned Hyperparameters Accuracy: \", logreg_cv.best_score_)\n",
    "print('\\nClasification report:\\n', classification_report(valid_stemy, logreg_pred))\n",
    "print(\"true negative: \", tn, \"\\n\")\n",
    "print(\"false positive: \", fp, \"\\n\")\n",
    "print(\"false negative: \", fn, \"\\n\")\n",
    "print(\"true positive: \", tp, \"\\n\")\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter review :this is bad\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Loading Model # \n",
    "filename1 = 'classification_logreg.pkl'\n",
    "\n",
    "with open(filename1, 'rb') as file:  \n",
    "    logistic = pickle.load(file)[0]\n",
    "    \n",
    "with open(filename1, 'rb') as file:  \n",
    "    user_uni = pickle.load(file)[1]\n",
    "\n",
    "user_input = input(\"Enter review :\")\n",
    "user_input = user_input.split()\n",
    "\n",
    "# Remove stop words # \n",
    "user_input = [word for word in user_input if word not in stop_list]\n",
    "\n",
    "# Make words case-insensitive # \n",
    "user_input = [word.lower() for word in user_input]\n",
    "\n",
    "# Remove punctuations if any # \n",
    "user_input = [re.sub('[^\\w\\s]','', word) for word in user_input]\n",
    "\n",
    "# Stemming with NLTK # \n",
    "user_input = stem(user_input)\n",
    "\n",
    "# Turn arrays for each row in df['lemmetized'] into a string #: Needed to run SkLearn Lib\n",
    "user_input = \" \".join(user_input)\n",
    "user_input = pd.Series(user_input)\n",
    "\n",
    "user_unigram = user_uni.transform(user_input)\n",
    "\n",
    "#print(user_input)\n",
    "#print(valid_stemx)\n",
    "\n",
    "prediction = logistic.predict(user_unigram)\n",
    "\n",
    "if prediction[0] == 1: \n",
    "    print(\"Positive\")\n",
    "else: \n",
    "    print(\"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_review(\"Hello this is great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.7"
=======
   "version": "3.7.4"
>>>>>>> 0390795af5ab677703548136401898fedd3010c2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
