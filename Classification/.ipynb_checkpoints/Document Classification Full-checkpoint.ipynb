{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries # \n",
    "import pandas as pd \n",
    "import nltk as nltk\n",
    "import  xgboost, numpy, string\n",
    "import datetime as dt\n",
    "import re as re\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions # \n",
    "\n",
    "# Timer to check execution timing for each function call # \n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = dt.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((dt.datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "def stem(array):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in array]\n",
    "\n",
    "def lemmetize(array):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    return [lemmatizer.lemmatize(w) for w in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Data Prep # \n",
    "df = pd.read_csv(\"data/cleaned_hotelreviews_short.csv\")\n",
    "\n",
    "# Drop rows with null comments # \n",
    "df = df.dropna(subset=['reviews'])\n",
    "\n",
    "# Make words case-insensitive # \n",
    "df = df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Accuracy:  0.945\n",
      "Logistic Regression Accuracy:  0.95\n",
      "SVM Accuracy:  0.76\n",
      "Random Forest Accuracy:  0.915\n",
      "Naive Bayes Accuracy:  0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run baseline models based on Count Vector # \n",
    "\n",
    "# Train - Test Split # \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['reviews'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable # \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# Create Count Vector #  \n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vector.fit(df['reviews'])\n",
    "\n",
    "# Transform training and validation data # \n",
    "xtrain_count = count_vector.transform(train_x)\n",
    "xvalid_count = count_vector.transform(valid_x)\n",
    "\n",
    "# XG Boost on Unprocessed Data # \n",
    "model = xgboost.XGBClassifier().fit(xtrain_count.tocsc(), train_y)\n",
    "predictions = model.predict(xvalid_count.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"XGBoost Classifier Accuracy: \", accuracy)\n",
    "\n",
    "# Logistic Regression on Unprocessed Data # \n",
    "model = linear_model.LogisticRegression().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Logistic Regression Accuracy: \", accuracy)\n",
    "\n",
    "# SMV on Unprocessed Data # \n",
    "model = svm.SVC().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"SVM Accuracy: \", accuracy)\n",
    "\n",
    "# Random Forest on Unprocessed Data # \n",
    "model  = ensemble.RandomForestClassifier().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Random Forest Accuracy: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Unprocessed Data # \n",
    "model = naive_bayes.MultinomialNB().fit(xtrain_count, train_y)\n",
    "predictions = model.predict(xvalid_count)\n",
    "accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "print(\"Naive Bayes Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Data Prep on two best models # \n",
    "\n",
    "# Remove punctuations if any # \n",
    "df[\"words_only\"] = df['reviews'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Remove stop words # \n",
    "stop_list = stopwords.words('english')\n",
    "df['reviews'] = df['reviews'].apply(lambda x: [word for word in x.split() if word not in stop_list])\n",
    "\n",
    "# Remove single words # \n",
    "df['reviews'] = df['reviews'].apply(lambda x: x if len(x) > 1 else [])\n",
    "\n",
    "# Drop rows where reviews == [] # \n",
    "df = df[df.reviews.str.len()>0]\n",
    "\n",
    "# Tokenization with NLTK # \n",
    "df['tokenized'] = df['words_only'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Stemming with NLTK # \n",
    "df['stemmed'] = df['tokenized'].apply(stem)\n",
    "\n",
    "# Turn arrays for each row in df['stemmed'] into a string #: Needed to run SkLearn Lib\n",
    "df['stemmed'] = df['stemmed'].apply(\" \".join)\n",
    "\n",
    "# Lemmetisation # \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "df['lemmetized'] = df['tokenized'].apply(lemmetize)\n",
    "\n",
    "# Turn arrays for each row in df['lemmetized'] into a string #: Needed to run SkLearn Lib\n",
    "df['lemmetized'] = df['lemmetized'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test Split for Stemmed Words # \n",
    "train_stemx, valid_stemx, train_stemy, valid_stemy = model_selection.train_test_split(df['stemmed'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable [STEMMED] # \n",
    "train_stemy = encoder.fit_transform(train_stemy)\n",
    "valid_stemy = encoder.fit_transform(valid_stemy)\n",
    "\n",
    "# Train - Test Split for Lemmetized Words # \n",
    "train_lemx, valid_lemx, train_lemy, valid_lemy = model_selection.train_test_split(df['lemmetized'], df['class'], train_size=0.8, random_state = 3000)\n",
    "\n",
    "# Label encode target variable [LEMMETIZED] # \n",
    "train_lemy = encoder.fit_transform(train_lemy)\n",
    "valid_lemy = encoder.fit_transform(valid_lemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_____________' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-ab6b920533a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Model 1 #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_____________\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_count_stem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_stemy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalid_count_stem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_stemy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_____________' is not defined"
     ]
    }
   ],
   "source": [
    "# Best Models [STEMMED] # \n",
    "\n",
    "# Create Count Vector [STEMMED] #  \n",
    "count_vector.fit(df['stemmed'])\n",
    "xtrain_count_stem = count_vector.transform(train_stemx)\n",
    "xvalid_count_stem = count_vector.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_count_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_count_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_count_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_count_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 2: \", accuracy)\n",
    "\n",
    "# Ngram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_quadgram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(4,4), max_features=5000)\n",
    "\n",
    "# Unigram [STEMMED] # \n",
    "tfidf_unigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_unigram_stem = tfidf_unigram.transform(train_stemx)\n",
    "xvalid_tfidf_unigram_stem = tfidf_unigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_tfidf_unigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_tfidf_unigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 2: \", accuracy)\n",
    "\n",
    "# Bigram [STEMMED] # \n",
    "tfidf_bigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_bigram_stem = tfidf_bigram.transform(train_stemx)\n",
    "xvalid_tfidf_bigram_stem = tfidf_bigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_tfidf_bigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_tfidf_bigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 2: \", accuracy)\n",
    "\n",
    "# Trigram [STEMMED] # \n",
    "tfidf_trigram.fit(df['stemmed'])\n",
    "xtrain_tfidf_trigram_stem = tfidf_trigram.transform(train_stemx)\n",
    "xvalid_tfidf_trigram_stem = tfidf_trigram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_tfidf_trigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_tfidf_bigram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 2: \", accuracy)\n",
    "\n",
    "# Quadgram [STEMMED] # \n",
    "tfidf_quadgram.fit(df['stemmed'])\n",
    "xtrain_tfidf_quadgram_stem = tfidf_quadgram.transform(train_stemx)\n",
    "xvalid_tfidf_quadgram_stem = tfidf_quadgram.transform(valid_stemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_tfidf_quadgram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_tfidf_quadgram_stem.tocsc(), train_stemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_stem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_stemy)\n",
    "print(\"Model 2: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_____________' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-e8ea5be3b727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Model 1 #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_____________\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_count_lem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_lemy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalid_count_lem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_lemy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_____________' is not defined"
     ]
    }
   ],
   "source": [
    "# Best Models [LEMMETIZED] # \n",
    "\n",
    "# Create Count Vector [LEMMETIZED] #  \n",
    "count_vector.fit(df['lemmetized'])\n",
    "xtrain_count_lem = count_vector.transform(train_lemx)\n",
    "xvalid_count_lem = count_vector.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_count_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_count_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_count_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_count_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 2: \", accuracy)\n",
    "\n",
    "# Ngram Level TF-IDF #: Matrix represents tf-idf scores of unigram (all terms are separate)\n",
    "tfidf_unigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=5000)\n",
    "tfidf_bigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_trigram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(3,3), max_features=5000)\n",
    "tfidf_quadgram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(4,4), max_features=5000)\n",
    "\n",
    "# Unigram [LEMMETIZED] # \n",
    "tfidf_unigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_unigram_lem = tfidf_unigram.transform(train_lemx)\n",
    "xvalid_tfidf_unigram_lem = tfidf_unigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_tfidf_unigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_tfidf_unigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_unigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 2: \", accuracy)\n",
    "\n",
    "# Bigram [LEMMETIZED] # \n",
    "tfidf_bigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_bigram_lem = tfidf_bigram.transform(train_lemx)\n",
    "xvalid_tfidf_bigram_lem = tfidf_bigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_tfidf_bigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_tfidf_bigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_bigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 2: \", accuracy)\n",
    "\n",
    "# Trigram [LEMMETIZED] # \n",
    "tfidf_trigram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_trigram_lem = tfidf_trigram.transform(train_lemx)\n",
    "xvalid_tfidf_trigram_lem = tfidf_trigram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_tfidf_trigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_tfidf_trigram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_trigram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 2: \", accuracy)\n",
    "\n",
    "# Quadgram [LEMMETIZED] # \n",
    "tfidf_quadgram.fit(df['lemmetized'])\n",
    "xtrain_tfidf_quadgram_lem = tfidf_quadgram.transform(train_lemx)\n",
    "xvalid_tfidf_quadgram_lem = tfidf_quadgram.transform(valid_lemx)\n",
    "\n",
    "# Model 1 # \n",
    "model = _____________.fit(xtrain_tfidf_quadgram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 1: \", accuracy)\n",
    "\n",
    "# Model 2 # \n",
    "model = _____________.fit(xtrain_tfidf_quadgram_lem.tocsc(), train_lemy)\n",
    "predictions = model.predict(xvalid_tfidf_quadgram_lem.tocsc())\n",
    "accuracy = metrics.accuracy_score(predictions, valid_lemy)\n",
    "print(\"Model 2: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model for Future Use, Pickle # \n",
    "import pickle \n",
    "\n",
    "# Save the model to disk # \n",
    "filename1 = 'model_1.sav'\n",
    "pickle.dump(filename1, open(filename1, 'wb'))\n",
    "\n",
    "filename2 = 'model_2.sav'\n",
    "pickle.dump(filename2, open(filename2, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Model # \n",
    "load_model_1 = pickle.load(open(filename1, 'rb'))\n",
    "load_model_2 = pickle.load(open(filename2, 'rb'))\n",
    "\n",
    "#result = load_model_1.score(X_test, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter review :hello this place sucks\n",
      "0    hello place suck\n",
      "dtype: object\n",
      "  (0, 1308)\t1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-7d1b4bc07c9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0muser_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muser_predictions\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[0;32m    738\u001b[0m                 self.class_log_prior_)\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"toarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter review :\")\n",
    "user_input = user_input.split()\n",
    "\n",
    "# Remove stop words # \n",
    "user_input = [word for word in user_input if word not in stop_list]\n",
    "\n",
    "# Make words case-insensitive # \n",
    "user_input = [word.lower() for word in user_input]\n",
    "\n",
    "# Remove punctuations if any # \n",
    "user_input = [re.sub('[^\\w\\s]','', word) for word in user_input]\n",
    "\n",
    "\n",
    "# Spelling checker # : Replace incorrect words with correct words \n",
    "#user_input = spell_correct(user_input)\n",
    "\n",
    "# Stemming with NLTK # \n",
    "user_input = stem(user_input)\n",
    "\n",
    "# Lemmetisation # \n",
    "user_input = lemmetize(user_input)\n",
    "\n",
    "# Turn arrays for each row in df['lemmetized'] into a string #: Needed to run SkLearn Lib\n",
    "user_input = \" \".join(user_input)\n",
    "user_input = pd.Series(user_input)\n",
    "\n",
    "print(user_input)\n",
    "\n",
    "# Count Vector # \n",
    "user_count = count_vector.transform(user_input)\n",
    "\n",
    "print(user_count)\n",
    "\n",
    "user_predictions = model.predict(user_count.tocsc())\n",
    "\n",
    "if user_predictions == 0:\n",
    "    print(\"Negative\")\n",
    "else:\n",
    "    print(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
